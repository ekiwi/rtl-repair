#!/usr/bin/env python3
# Copyright 2022-2023 The Regents of the University of California
# released under BSD 3-Clause License
# author: Kevin Laeufer <laeufer@cs.berkeley.edu>
#
# uses VCD traces generated from benchmarks to calculate the
# output/state divergence delta (OSDD) for different bug scenarios
import subprocess
import sys
import argparse
from dataclasses import dataclass
from pathlib import Path
import typing

# add root dir in order to be able to load "benchmarks" module
_script_dir = Path(__file__).parent.resolve()
sys.path.append(str(_script_dir.parent))
import benchmarks
from benchmarks import Benchmark, load_project, get_benchmark, get_benchmark_design
from find_state import find_state_and_outputs

# list of benchmarks that are out of scope for this metric since they are non synthesizable
out_of_scope = {
    ('first_counter_overflow', 'wadden_buggy1'): "missing posedge",
    ('lshift_reg', 'kgoliya_buggy1'): "posedge changed to negedge",
    ('i2c_slave', 'wadden_buggy1'): "non-synthesizable testbench model",
    ('i2c_slave', 'wadden_buggy2'): "non-synthesizable testbench model",
    ('mux_4_1', 'wadden_buggy1'): "buggy design has a latch, but no clock that we can use to sample",
    ('mux_4_1', 'wadden_buggy2'): "buggy design has a latch, but no clock that we can use to sample",
}

osdd_binary = _script_dir / "osdd" / "target" / "release" / "osdd"
assert osdd_binary.is_file(), "Cannot find the osdd calculator. Please run `cargo build --release` in scripts/osdd"

@dataclass
class Config:
    working_dir: Path
    benchmark: Benchmark


def assert_exists(filename: Path):
    assert filename.exists(), f"{filename} ({filename.resolve()}) does not exist!"


@dataclass
class Disagreement:
    step: int
    signal: str
    is_state: bool
    is_output: bool
    expected: str
    actual: str


@dataclass
class Result:
    project: str
    bug: str
    delta: int
    first_output_disagreement: int
    ground_truth_testbench_cycles: int
    notes: str
    warnings: list[str]

def write_osdd_toml(filename: Path, results: list[Result]):
    with open(filename, 'w') as ff:
        print(f"# generated by a script: {__file__}", file=ff)
        for res in results:
            print("", file=ff)
            print("[[osdds]]", file=ff)
            print(f'project="{res.project}"', file=ff)
            print(f'bug="{res.bug}"', file=ff)
            print(f'delta={res.delta}', file=ff)
            print(f'first_output_disagreement={res.first_output_disagreement}', file=ff)
            print(f'ground_truth_testbench_cycles={res.ground_truth_testbench_cycles}', file=ff)
            print(f'notes="{res.notes}"', file=ff)
            warnings = "[ " + ", ".join(f'"{w}"' for w in res.warnings) + " ]"
            print(f'warnings={warnings}', file=ff)

_logfile: typing.Optional[typing.TextIO] = None


def info(msg: str):
    if _logfile is None:
        print(msg)
    else:
        _logfile.write(msg + "\n")


def start_logger(logfile: typing.Optional[Path]):
    global _logfile
    if logfile is not None:
        _logfile = open(logfile, "w")


def end_logger():
    global _logfile
    if _logfile is not None:
        _logfile.close()


def common_list(a: list, b: list) -> list:
    return sorted(list(set(a) & set(b)))


def union_list(a: list, b: list) -> list:
    return sorted(list(set(a) | set(b)))


def filer_mem_regs(states: list) -> list:
    """ yosys will generate some registers that serve to hold memory read or write signals, we chose to ignore these here """
    return [st for st in states if '/' not in st[0]]


def compare_traces(conf: Config) -> Result:
    res = Result(project=conf.benchmark.project_name, bug=conf.benchmark.bug.name,
                 delta=-1, first_output_disagreement=-1, ground_truth_testbench_cycles=-1,
                 notes="", warnings=[])

    # check to see if this is a project where our OSDD metric does not make sense
    if (res.project, res.bug) in out_of_scope:
        res.notes = "OSDD does not apply because " + out_of_scope[(res.project, res.bug)]
        return res

    # extract state and outputs from ground truth design
    gt_design = conf.benchmark.design
    gt_states, gt_outputs = find_state_and_outputs(conf.working_dir, gt_design)
    gt_states = filer_mem_regs(gt_states)

    # extract state and outputs from buggy design
    buggy_design = get_benchmark_design(conf.benchmark)
    buggy_states, buggy_outputs = find_state_and_outputs(conf.working_dir, buggy_design)
    buggy_states = filer_mem_regs(buggy_states)

    # if there is no state in the design, OSDD is always 0
    gt_no_state, buggy_no_state = len(gt_states) == 0, len(buggy_states) == 0
    if gt_no_state and buggy_no_state:
        res.delta = 0
        res.notes = "no state => delta=0"
        return res

    # compare states, see if they are the same
    if not gt_states == buggy_states:
        # if the buggy design adds state, we can try to see if (i.e. hope that) the same signal
        # exists in the ground truth design as a signal wire
        # TODO: how is the repair for something like this represented in the synthesized design
        #       with change templates applied?
        only_additional_buggy_states = set(gt_states).issubset(set(buggy_states))
        if only_additional_buggy_states:
            gt_states = buggy_states
        else:
            states_missing_from_buggy = set(gt_states) - set(buggy_states)
            print(f"WARN: states are not the same!\nMissing states in buggy design: {states_missing_from_buggy}")
            res.warnings.append("states are not the same!")
            res.warnings.append(f"Missing states in buggy design: {states_missing_from_buggy}")
            buggy_states = gt_states

    # display warning if outputs are not the same
    if not gt_outputs == buggy_outputs:
        print(f"WARN: outputs are not the same")
        print(f"Missing in buggy: {list(set(gt_outputs) - set(buggy_outputs))}")
        print(f"Additional in buggy: {list(set(buggy_outputs) - set(gt_outputs))}")
        res.warnings.append("outputs are not the same")
        res.warnings.append(f"Missing in buggy: {list(set(gt_outputs) - set(buggy_outputs))}")
        res.warnings.append(f"Additional in buggy: {list(set(buggy_outputs) - set(gt_outputs))}")

    # we are only interested in signals that are contained in both circuits, but the widths are allowed to differ
    interesting_states = common_list([n for n, _ in gt_states], [n for n, _ in buggy_states])
    interesting_outputs = common_list([n for n, _ in gt_outputs], [n for n, _ in buggy_outputs])
    # interesting_signals = union_list(interesting_states, interesting_outputs)

    # VCD names used in the `generate_vcd_traces.py` script
    gt_vcd = conf.working_dir / f"{conf.benchmark.project_name}.groundtruth.vcd"
    buggy_vcd = conf.working_dir / f"{conf.benchmark.project_name}.{conf.benchmark.bug.name}.vcd"
    assert_exists(gt_vcd)
    assert_exists(buggy_vcd)

    # derive logfile name from benchmark
    logfile: Path = conf.working_dir / f"{conf.benchmark.project_name}.{conf.benchmark.bug.name}.log"

    # write signals to file
    signal_file: Path = conf.working_dir / f"{conf.benchmark.project_name}.{conf.benchmark.bug.name}.signals"
    with open(signal_file, 'w') as f:
        print(", ".join(interesting_states), file=f)
        print(", ".join(interesting_outputs), file=f)

    # call to rust binary in order to quickly compare VCDs
    cmd = [str(osdd_binary.resolve()),
           f"--gt-wave={gt_vcd.resolve()}",
           f"--buggy-wave={buggy_vcd.resolve()}",
           f"--signals={signal_file.resolve()}",
    ]
    # print(" ".join(cmd))
    r = subprocess.run(cmd, check=True, stdout=subprocess.PIPE)
    first_state, first_output = [int(p) for p in r.stdout.decode('utf-8').split(',')]

    if first_output == -1:
        res.notes = "no disagreement found"
    else:
        assert first_state <= first_output
        res.first_output_disagreement = first_output
        if first_state == -1:
            delta = 0
        else:
            delta = first_output - first_state + 1
        res.delta = delta

    return res


def print_result(res: Result):
    note = "" if len(res.notes) == 0 else f" ({res.notes})"
    print(f"{res.project}.{res.bug}: failed at {res.first_output_disagreement} w/ osdd={res.delta}{note}")


def parse_args() -> Config:
    parser = argparse.ArgumentParser(description='Calculate output / state divergence delta')
    parser.add_argument('--working-dir', dest='working_dir',
                        help='Working directory. Should contain the VCD traces produced by `generate_vcd_traces.py`',
                        required=True)
    parser.add_argument('--project', help='Project TOML')
    parser.add_argument('--bug', help='Bug name.')

    args = parser.parse_args()
    if args.project is None:
        benchmark = None
        assert args.bug is None, f"Cannot specify a bug `{args.bug}` without a project!"
    else:
        assert args.bug is not None, f"Need to specify a bug together with the project!"
        project = load_project(Path(args.project))
        benchmark = get_benchmark(project, args.bug)

    conf = Config(Path(args.working_dir), benchmark)

    assert_exists(conf.working_dir)
    return conf

def diff_all(working_dir: Path):
    print("No project+bug specified, thus we are diffing all CirFix benchmarks.")
    projects = benchmarks.load_all_projects()
    results = []

    for proj in projects.values():
        gt_vcd = working_dir / f"{proj.name}.groundtruth.vcd"
        if not gt_vcd.exists():
            print(f"{gt_vcd} does not exist. Skipping project `{proj.name}`")
            continue
        for bb in benchmarks.get_benchmarks(proj):
            # skip bugs that are not part of the cirfix paper
            if not benchmarks.is_cirfix_paper_benchmark(bb):
                continue
            # check to see if the VCD is available:
            buggy_vcd = working_dir / f"{proj.name}.{bb.bug.name}.vcd"
            if not buggy_vcd.exists():
                print(f"{buggy_vcd} does not exist. Skipping bug `{bb.bug.name}` in project `{proj.name}`")
                continue
            conf = Config(working_dir, bb)
            print(f"Comparing traces for {proj.name} {bb.bug.name}")
            res = compare_traces(conf)
            print_result(res)
            results.append(res)

    osdd_toml = working_dir / "osdd.toml"
    write_osdd_toml(osdd_toml, results)
    print(f"Wrote results to: {osdd_toml}")
    return results


def main():
    conf = parse_args()

    if conf.benchmark is None:
        diff_all(conf.working_dir)
    else:
        res = compare_traces(conf)
        print_result(res)


if __name__ == '__main__':
    main()
