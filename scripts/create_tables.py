#!/usr/bin/env python3
# Copyright 2023 The Regents of the University of California
# released under BSD 3-Clause License
# author: Kevin Laeufer <laeufer@cs.berkeley.edu>
#
# creates the tables for the evaluation section of the RTL-Repair paper

import sys
import argparse
from pathlib import Path
from dataclasses import dataclass

# add root dir in order to be able to load "benchmarks" module
_script_dir = Path(__file__).parent.resolve()
sys.path.append(str(_script_dir.parent))
import benchmarks
from benchmarks import assert_file_exists, assert_dir_exists

# map benchmark to short name
project_short_names = {
    "decoder_3_to_8": "decoder",
    "first_counter_overflow": "counter",
    "flip_flop": "flop",
    "fsm_full": "fsm",
    "lshift_reg": "shift",
    "mux_4_1": "mux",
    "i2c_slave": "i2c",
    "i2c_master": "i2c",
    "sha3_keccak": "sha3",
    "sha3_padder": "sha3",
    "pairing": "pairing",
    "reed_solomon_decoder": "reed",
    "sdram_controller": "sdram",
}
to_short_name = {}
def _calc_short_names():
    for project_name, entry in benchmarks.benchmark_to_cirfix_paper_table_3.items():
        to_short_name[project_name] = {}
        short = project_short_names[project_name]
        for bug in entry.keys():
            to_short_name[project_name][bug] = short + "_" + bug[0] + bug[-1]
_calc_short_names()
def get_short_name(project: str, bug: str):
    return to_short_name[project][bug]

@dataclass
class Config:
    working_dir: Path
    osdd_toml: Path


def parse_args() -> Config:
    parser = argparse.ArgumentParser(description='Generate tables for the RTL-Repair paper')
    parser.add_argument('--working-dir', help='Output directory.', required=True)
    parser.add_argument('--osdd-toml', help='Path to osdd.toml generated by calc_osdd.py.', required=True)



    args = parser.parse_args()
    conf = Config(Path(args.working_dir), Path(args.osdd_toml))

    assert_dir_exists("working directory parent", conf.working_dir.parent)
    assert_file_exists("osdd toml", conf.osdd_toml)
    return conf

def _render_latex_row(column_width: list[int], row: list[str]) -> str:
    padded = [str(cell).ljust(width, ' ') for width, cell in zip(column_width, row)]
    return " & ".join(padded)

def render_latex(table: list[list[str]], has_header: bool) -> str:
    if len(table) == 0:
        return ""

    # determine number and size of columns
    column_width = [0] * len(table[0])
    for row in table:
        assert len(row) == len(column_width),\
        f"Expected all rows to have {len(column_width)} columns, but this one has {len(row)}"
        for ii, cell in enumerate(row):
            column_width[ii] = max(column_width[ii], len(str(cell)))

    if has_header:
        header = table[0]
        table = table[1:]

    rows = [_render_latex_row(column_width, row) for row in table]
    row_sep = " \\\\\n"
    table_str = row_sep.join(rows) + "\n"

    if has_header:
        table_str = _render_latex_row(column_width, header) + " \\\\ \\midrule\n" + table_str

    return table_str


def write_to(filename: Path, content: str):
    with open(filename, 'w') as ff:
        ff.write(content)


def benchmark_description_table(conf: Config) -> list[list[str]]:
    header = ["Project", "Defect", "Short Name"]
    rows = [header]
    for project_name, entry in benchmarks.benchmark_to_cirfix_paper_table_3.items():
        for bug, (description, _category, _time, _status) in entry.items():
            row = [project_name, description, get_short_name(project_name, bug)]
            rows.append(row)
    return rows
def main():
    conf = parse_args()

    # create working directory if it does not exist already
    if not conf.working_dir.exists():
        conf.working_dir.mkdir()

    write_to(conf.working_dir / "benchmark_description_table.tex",
             render_latex(benchmark_description_table(conf), has_header=True))





if __name__ == '__main__':
    main()
